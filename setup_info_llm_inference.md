## LLM Inferencing Strategy

#### Cloud Model and API
- OpenAI ([pricing](https://openai.com/api/pricing/))
- AWS Bedrock ([pricing](https://aws.amazon.com/bedrock/pricing/))
- Google AI Studio
- NVIDIA AI ([info](https://build.nvidia.com/))

#### Self-hosted Model and API
- Ollama ([Github](https://github.com/ollama/ollama), [Website](https://ollama.com/))

#### Cloud GPU Machine
- AWS EC2

#### AI Oriented UI
- Open WebUI ([Github](https://github.com/open-webui/open-webui), [Website](https://openwebui.com/))
- Gradio ([Github](https://github.com/gradio-app/gradio), [Website](https://www.gradio.app/))
- Streamlit ([Github](https://github.com/streamlit/streamlit), [Website](https://streamlit.io/))

#### AI System Development Platform
- Dify ([Github](https://github.com/langgenius/dify/), [Website](https://dify.ai/))
